{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install pyarrow\n",
    "# !pip install geopandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install pyproj\n",
    "# !pip install geopy\n",
    "# !pip install \"geopy[aiohttp]\"\n",
    "# !pip install tqdm\n",
    "# !pip install pycountry\n",
    "import logging\n",
    "import os\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import pytz\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from geopy.adapters import AioHTTPAdapter\n",
    "from geopy.exc import GeocoderServiceError, GeocoderTimedOut\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This functions converts string likes POLYGON((x.xxx x.xxxx, x.xxx x.xxxx, x.xxx x.xxxx, x.xxx x.xxxx, x.xxx x.xxxx))\n",
    "to tuple out of the first point\n",
    "\"\"\"\n",
    "def transform_polygon_string_to_tuple(polygon_string):\n",
    "    subst = \"\"\n",
    "    polygon_regex = r\"(POLYGON\\(\\()(-?\\d*\\.?\\d*\\s-?\\d*\\.?\\d*)\"\n",
    "    result = re.search(polygon_regex, polygon_string)\n",
    "    if result == None:\n",
    "        print(polygon_string)\n",
    "    polygon_string = result.group(2)\n",
    "    return_tuple = tuple(map(int, map(float, polygon_string.split(' '))))\n",
    "    return return_tuple\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function \n",
    "1. walk through the whole dataset\n",
    "2. reads parquet files\n",
    "3. transform values in the columns 'tile' from the parquet file using \n",
    "transform_polygon_string_to_tuple() into a tuple(long,lat) to enable us\n",
    "query the country based on the column value\n",
    "4. make a unique list of the generated tuples\n",
    "5. write those tuples to a new .csv file where the column country is MBD(placeholder)\n",
    "6. after finishing the walk through, removes duplicate values from the csv file \n",
    "\"\"\"\n",
    "def build_coordinate_db(db_file):\n",
    "    db = pd.DataFrame(columns=['long_lat', 'country'])\n",
    "    db.long_lat = ''\n",
    "    db.country = ''\n",
    "    db.to_csv(db_file, index=False, encoding='utf-8')\n",
    "    for subdir, dirs, files in os.walk('dataset'):\n",
    "        for file in files:\n",
    "            if file.endswith('parquet'):\n",
    "                file_name = os.path.join(subdir, file)\n",
    "                print(\"Reading now from \" + file_name)\n",
    "                source_parquet = pd.read_parquet(file_name, engine='pyarrow')\n",
    "\n",
    "                print(\"Converting 5 points polygons to 1 point\")\n",
    "                list_of_tuples_tiles = source_parquet.tile.apply(\n",
    "                    transform_polygon_string_to_tuple)\n",
    "                print(\"Getting the unique values of the polygons\")\n",
    "                unique_tiles = list(set(list_of_tuples_tiles))\n",
    "                print(\"Writing the unique values to CSV file \")\n",
    "                db = pd.DataFrame({'long_lat': unique_tiles, 'country': 'MBD'})\n",
    "                db.to_csv(db_file,\n",
    "                          mode='a',\n",
    "                          index=False,\n",
    "                          header=False,\n",
    "                          encoding='utf-8')\n",
    "                print(\"Finished extracting unique coordinate points for \" +\n",
    "                      file_name)\n",
    "                print(\"\\n\")\n",
    "\n",
    "    print(\"Removing duplicate values from the csv\")\n",
    "    db = pd.read_csv(db_file)\n",
    "    db.drop_duplicates(subset=None, inplace=True)\n",
    "    db.to_csv(db_file, index=False)\n",
    "    print(\"Finished creating the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function performs the actual call to geoAPI to fetch a country name for a given (long,lat)\n",
    "\"\"\"\n",
    "def perform_geo_reverse_request(geolocator, lat_long_point):\n",
    "    country = 'undefined'\n",
    "    location = geolocator.reverse(lat_long_point, timeout=5, language=\"en\")\n",
    "    if location != None:\n",
    "        country = location.raw.get('address').get('country')\n",
    "        if country == None:\n",
    "            # For places like Gulf of Thailand, Bermuda Triangle and VETERINARY STATION MALI LOŠINJ \n",
    "            country = location.raw.get('address').get('locality')\n",
    "            if country == None:\n",
    "                # Sometimes we have only information about a location including country code, then use pytz to extract the country name\n",
    "                country = pytz.country_names[location.raw.get('address').get('country_code')]\n",
    "    return country\n",
    "\n",
    "\"\"\" \n",
    "This function takes a tuple in and return the location using  Nominatim api.\n",
    "Places middle in the ocean would take country name as undefined. Later we can decide what to do\n",
    "with those values \n",
    "\"\"\"\n",
    "def convert_single_polygon_to_country(poly_item):\n",
    "    logger = logging.getLogger()\n",
    "    country = 'undefined'\n",
    "    (longitude, latitude) = poly_item\n",
    "    my_user_agent = 'UTwente_Managing_Big_Data_project_{}'.format(randint(10000, 99999))\n",
    "    \n",
    "    #OpenstreetMap API\n",
    "    geolocator = Nominatim(user_agent=my_user_agent)\n",
    "    #OpenStreetMap API expects first latitude and then longitude\n",
    "    lat_long_point = str(latitude) + \",\" + str(longitude)\n",
    "    try:\n",
    "        country = perform_geo_reverse_request(geolocator, lat_long_point)\n",
    "    except GeocoderTimedOut:\n",
    "        logger.error('ERROR: GeocoderTimedOut, we will keep retrying...')\n",
    "        sleep(randint(1 * 100, 5 * 100) / 100)\n",
    "        country = perform_geo_reverse_request(geolocator, lat_long_point)\n",
    "    except GeocoderServiceError as e:\n",
    "        logger.error('CONNECTION ERROR: GeocoderServiceError {}'.format(e) )\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error('ERROR: Terminating due to an exception {}'.format(e))\n",
    "        logger.error(str(poly_item))\n",
    "        logger.error(e)\n",
    "        return None\n",
    "    return country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This functions takes csv file with two columns long_lat and country \n",
    "and replace the values of the country columns with the country name corresponding to\n",
    "the tuple of (long, lat) in long_lat column\"\"\"\n",
    "\n",
    "\n",
    "def fill_coordinate_db_with_countries(source_file, target_file):\n",
    "    db = pd.read_csv(source_file, sep=',')\n",
    "    db.long_lat = db.long_lat.apply(lambda x: literal_eval(x))\n",
    "    index = 0\n",
    "    for i in tqdm(range(0, db.shape[0])):\n",
    "        db.at[i, 'country'] = convert_single_polygon_to_country(db.at[i, 'long_lat'])\n",
    "        try:\n",
    "            # db.at[i, 'country'] = convert_single_polygon_to_country(db.at[i, 'long_lat'])\n",
    "            index = index + 1\n",
    "        except Exception as e:\n",
    "            print('ERROR: because  {}'.format(e))\n",
    "    print(str(index) + \" rows have been processed\")\n",
    "    db.to_csv(target_file, sep=',', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function takes a newly created db file from fill_coordinate_db_with_countries and \n",
    "checks if there are any missing values in the column country and tries to get the corresponding country name\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def check_if_built_coordinate_db_contains_missing_values(db_file):\n",
    "    db = pd.read_csv(db_file, sep=',')\n",
    "    # Convert the string representation of tuples to real tuples double check\n",
    "    db.long_lat = db.long_lat.apply(lambda x: literal_eval(x))\n",
    "    index = 0\n",
    "    for i in tqdm(range(0, db.shape[0])):\n",
    "        #MBD was used first in the code as placeholder. Whenever the country is empty or it's null, then it needs to be re-queried\n",
    "        if (db.at[i, 'country'] == 'MBD' or pd.isnull(db.at[i, 'country'])):\n",
    "            try:\n",
    "                db.at[i, 'country'] = convert_single_polygon_to_country(\n",
    "                    db.at[i, 'long_lat'])\n",
    "                index = index + 1\n",
    "            except Exception as e:\n",
    "                print('ERROR: because  {}'.format(e))\n",
    "\n",
    "    print(str(index) + \" rows have been processed\")\n",
    "    db.to_csv(db_file, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the countries database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading now from dataset/2019-q1/2019-01-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q1/2019-01-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q1/2019-01-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q1/2019-01-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q2/2019-04-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q2/2019-04-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q2/2019-04-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q2/2019-04-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q3/2019-07-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q3/2019-07-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q3/2019-07-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q3/2019-07-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q4/2019-10-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q4/2019-10-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2019-q4/2019-10-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2019-q4/2019-10-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q1/2020-01-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q1/2020-01-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q1/2020-01-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q1/2020-01-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q2/2020-04-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q2/2020-04-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q2/2020-04-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q2/2020-04-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q3/2020-07-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q3/2020-07-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q3/2020-07-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q3/2020-07-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q4/2020-10-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q4/2020-10-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2020-q4/2020-10-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2020-q4/2020-10-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q1/2021-01-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q1/2021-01-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q1/2021-01-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q1/2021-01-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q2/2021-04-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q2/2021-04-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q2/2021-04-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q2/2021-04-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q3/2021-07-01_performance_fixed_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q3/2021-07-01_performance_fixed_tiles.parquet\n",
      "\n",
      "\n",
      "Reading now from dataset/2021-q3/2021-07-01_performance_mobile_tiles.parquet\n",
      "Converting 5 points polygons to 1 point\n",
      "Getting the unique values of the polygons\n",
      "Writing the unique values to CSV file \n",
      "Finished extracting unique coordinate points for dataset/2021-q3/2021-07-01_performance_mobile_tiles.parquet\n",
      "\n",
      "\n",
      "Removing duplicate values from the csv\n",
      "Finished creating the database\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(filename=\"logger.log\", format='%(asctime)s %(message)s \\n', filemode='w')\n",
    "# Build the database from the dataset \n",
    "build_coordinate_db('dataset/long_lat_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch countries names for each given tuple(long,lat) in the database file \n",
    "# Executing this line will call the OpenStreetMap API 13200 times. Be careful :)  \n",
    "fill_coordinate_db_with_countries('dataset/long_lat_db.csv','dataset/long_lat_db_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:00<00:00, 12287.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows have been processed so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any missing values in the newely built database files \n",
    "check_if_built_coordinate_db_contains_missing_values('dataset/long_lat_db_filled.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5d5ac7a7bfad39d13684607cb7a8589ceef353fd9fd58484c2060c3ec276197"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('python3613': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
